{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b703dde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b703dde",
    "outputId": "dbda6b1e-fc53-41bc-b274-4e213de07b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow_hub\n",
    "#!pip install transformers\n",
    "#!pip install --upgrade tensorflow-hub\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# 加载模型\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "#model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "#tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e1e17bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e1e17bd",
    "outputId": "613ffa47-85e6-4d01-f8dd-fdebbf8fe063"
   },
   "outputs": [],
   "source": [
    "\n",
    "#folder_path = \"C:/Users/25031/Desktop/新建文件夹/dataset_pt2/_PUBLICO/A_a_grecia_publico/SBERT/\"\n",
    "import os\n",
    "import torch\n",
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=10'\n",
    "from nltk.corpus import stopwords\n",
    "import gc\n",
    "Vec = []\n",
    "count = 0\n",
    "CON = []\n",
    "\n",
    "\n",
    "for count in range(0,67):\n",
    "    \n",
    "    # READ article file\n",
    "    article_path = \"C:/Users/25031/Desktop/新建文件夹/dataset_pt2/_PUBLICO/A_25_de_abril_publico/doc/\" + str(count) + \".txt\"\n",
    "    file = open(article_path, 'r', encoding='utf-8')\n",
    "    content = file.read()\n",
    "    content = content.replace(\"\\n\", \" \")\n",
    "    CON.append(content)\n",
    "    file.close()\n",
    "    count = count + 1\n",
    "    \n",
    "    \n",
    "# process by SBERT model\n",
    "outputs = model(CON)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9232789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "<class 'numpy.ndarray'>\n",
      "(67, 512)\n",
      "[[-0.05103007 -0.0354083  -0.04400484 ...  0.06518175 -0.04727732\n",
      "  -0.06521145]\n",
      " [ 0.02266826  0.01368557 -0.02627646 ...  0.06467165 -0.00466325\n",
      "  -0.0646777 ]\n",
      " [-0.04227672  0.01859396 -0.02096764 ...  0.06144354 -0.05205688\n",
      "  -0.06155482]\n",
      " ...\n",
      " [-0.01161882  0.01199497 -0.05013431 ...  0.0577615  -0.01380557\n",
      "  -0.06641843]\n",
      " [-0.03939731 -0.00056487 -0.00268506 ...  0.0558146  -0.04791026\n",
      "  -0.05581476]\n",
      " [-0.05040838  0.03028385 -0.02723645 ...  0.05788875 -0.04944053\n",
      "  -0.05788876]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(len(outputs))\n",
    "outputs = numpy.array(outputs)\n",
    "print(type(outputs))\n",
    "print(outputs.shape)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1cb91bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "# Extract data from TLS DATASET\n",
    "import os\n",
    "folder_path = \"C:/Users/25031/Desktop/新建文件夹/dataset_pt2/_PUBLICO/A_25_de_abril_publico/input_docs\"\n",
    "import os\n",
    "\n",
    "dct = []    # 用于存储文件名的列表\n",
    "\n",
    "# 遍历当前目录的所有子目录\n",
    "for subdir in os.listdir(folder_path):\n",
    "    subdir_path = os.path.join(folder_path, subdir)\n",
    "    # 如果是文件夹，则进一步遍历其下的文件\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, file_name)\n",
    "            # 确保是文件而不是文件夹\n",
    "            if os.path.isfile(file_path):\n",
    "                # 打开文件并读取内容\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    dct.append(file_name[0:10])\n",
    "\n",
    "print(len(dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5aafa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "similarity_matrix = cosine_similarity(outputs)\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "np.fill_diagonal(distance_matrix, 0)\n",
    "\n",
    "# 设置你想要的距离值\n",
    "large_distance = 100\n",
    "\n",
    "for i in range(len(dct)-1):\n",
    "    date_str1 = dct[i]\n",
    "    date_str2 = dct[i+1]\n",
    "    time1 = datetime.strptime(date_str1, \"%Y-%m-%d\")\n",
    "    time2 = datetime.strptime(date_str2, \"%Y-%m-%d\")\n",
    "    time_difference = abs(time2 - time1)   \n",
    "    \n",
    "    # 在距离矩阵中为这两篇文章设置距离\n",
    "    if time_difference > timedelta(days=1) :\n",
    "        distance_matrix[i][i+1] = large_distance\n",
    "        distance_matrix[i+1][i] = large_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b20267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_threshold:  1.1 Silhouette Coefficient:  0.032844413\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m clustering_model \u001b[38;5;241m=\u001b[39m AgglomerativeClustering(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, distance_threshold\u001b[38;5;241m=\u001b[39mdistance) \u001b[38;5;66;03m#, affinity='cosine', linkage='average', distance_threshold=0.4)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m clustering_model\u001b[38;5;241m.\u001b[39mfit_predict(vec_numpy)\n\u001b[1;32m----> 7\u001b[0m sil_coeff \u001b[38;5;241m=\u001b[39m silhouette_score(vec_numpy, labels, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_threshold: \u001b[39m\u001b[38;5;124m\"\u001b[39m, distance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSilhouette Coefficient: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sil_coeff)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:117\u001b[0m, in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:231\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    229\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m    230\u001b[0m label_freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(labels)\n\u001b[1;32m--> 231\u001b[0m check_number_of_labels(\u001b[38;5;28mlen\u001b[39m(le\u001b[38;5;241m.\u001b[39mclasses_), n_samples)\n\u001b[0;32m    233\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[0;32m    234\u001b[0m reduce_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m    235\u001b[0m     _silhouette_reduce, labels\u001b[38;5;241m=\u001b[39mlabels, label_freqs\u001b[38;5;241m=\u001b[39mlabel_freqs\n\u001b[0;32m    236\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:33\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[1;34m(n_labels, n_samples)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    Number of samples.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m n_labels \u001b[38;5;241m<\u001b[39m n_samples:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labels is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;241m%\u001b[39m n_labels\n\u001b[0;32m     36\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#for distance in [i / 10 for i in range(11, 25)]:\n",
    "#    clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=distance) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
    "#    clusters = clustering_model.fit_predict(distance_matrix)\n",
    "#    sil_coeff = silhouette_score(clusters, clusters, metric='euclidean')\n",
    "#    print(\"distance_threshold: \", distance, \"Silhouette Coefficient: \", sil_coeff)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82ae6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 25\n",
      "Cluster 1: 30\n",
      "Cluster 2: 31\n",
      "Cluster 3: 43\n",
      "Cluster 4: 27\n",
      "Cluster 5: 1\n",
      "Cluster 6: 29\n",
      "Cluster 7: 26\n",
      "Cluster 8: 8\n",
      "Cluster 9: 35\n",
      "Cluster 10: 44\n",
      "Cluster 11: 45\n",
      "Cluster 12: 37\n",
      "Cluster 13: 36\n",
      "Cluster 14: 40\n",
      "Cluster 15: 3\n",
      "Cluster 16: 4\n",
      "Cluster 17: 17\n",
      "Cluster 18: 41\n",
      "Cluster 19: 1\n",
      "Cluster 20: 32\n",
      "Cluster 21: 10\n",
      "Cluster 22: 8\n",
      "Cluster 23: 28\n",
      "Cluster 24: 7\n",
      "Cluster 25: 39\n",
      "Cluster 26: 7\n",
      "Cluster 27: 38\n",
      "Cluster 28: 9\n",
      "Cluster 29: 15\n",
      "Cluster 30: 3\n",
      "Cluster 31: 16\n",
      "Cluster 32: 2\n",
      "Cluster 33: 21\n",
      "Cluster 34: 3\n",
      "Cluster 35: 4\n",
      "Cluster 36: 8\n",
      "Cluster 37: 5\n",
      "Cluster 38: 18\n",
      "Cluster 39: 10\n",
      "Cluster 40: 34\n",
      "Cluster 41: 14\n",
      "Cluster 42: 19\n",
      "Cluster 43: 46\n",
      "Cluster 44: 42\n",
      "Cluster 45: 33\n",
      "Cluster 46: 0\n",
      "Cluster 47: 5\n",
      "Cluster 48: 4\n",
      "Cluster 49: 9\n",
      "Cluster 50: 2\n",
      "Cluster 51: 22\n",
      "Cluster 52: 13\n",
      "Cluster 53: 16\n",
      "Cluster 54: 0\n",
      "Cluster 55: 23\n",
      "Cluster 56: 12\n",
      "Cluster 57: 24\n",
      "Cluster 58: 6\n",
      "Cluster 59: 11\n",
      "Cluster 60: 2\n",
      "Cluster 61: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "similarity_matrix = cosine_similarity(outputs)\n",
    "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1)\n",
    "clusters = clustering_model.fit_predict(similarity_matrix)\n",
    "\n",
    "for i, cluster in enumerate(labels):\n",
    "    print(f\"Cluster {i}: {cluster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8e48000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 14  3  4 13 11 14  7  2  8  2 10  5  7  3 13  2 11  1 11  3  3  2 10\n",
      "  3  0  3  0  2  4 13  3  2  3 13  2  2 12  5  3  1  0  9 15 15 16  1 12\n",
      "  2  2  2 15  0  3  1  6  7  1 15  6  2  4]\n",
      "Number of clusters: 17\n"
     ]
    }
   ],
   "source": [
    "clustered_list = {}\n",
    "for article_id, cluster_id in enumerate(clusters):\n",
    "    if cluster_id not in clustered_list:\n",
    "        clustered_list[cluster_id] = []\n",
    "\n",
    "    clustered_list[cluster_id].append(article_id)\n",
    "\n",
    "file_path = \"C:/Users/25031/Desktop/新建文件夹/dataset_pt2/_PUBLICO/A_wuhan_publico/RESULT/\"\n",
    "numC = 1\n",
    "for cluster, n  in clustered_list.items():\n",
    "    print(\"Cluster \", n[0]+1)\n",
    "    print(cluster) \n",
    "    numC = numC+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "10ffd139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  6\n",
      "[0, 12, 38]\n",
      "Cluster  15\n",
      "[1, 6]\n",
      "Cluster  4\n",
      "[2, 14, 20, 21, 24, 26, 31, 33, 39, 53]\n",
      "Cluster  5\n",
      "[3, 29, 61]\n",
      "Cluster  14\n",
      "[4, 15, 30, 34]\n",
      "Cluster  12\n",
      "[5, 17, 19]\n",
      "Cluster  8\n",
      "[7, 13, 56]\n",
      "Cluster  3\n",
      "[8, 10, 16, 22, 28, 32, 35, 36, 48, 49, 50, 60]\n",
      "Cluster  9\n",
      "[9]\n",
      "Cluster  11\n",
      "[11, 23]\n",
      "Cluster  2\n",
      "[18, 40, 46, 54, 57]\n",
      "Cluster  1\n",
      "[25, 27, 41, 52]\n",
      "Cluster  13\n",
      "[37, 47]\n",
      "Cluster  10\n",
      "[42]\n",
      "Cluster  16\n",
      "[43, 44, 51, 58]\n",
      "Cluster  17\n",
      "[45]\n",
      "Cluster  7\n",
      "[55, 59]\n"
     ]
    }
   ],
   "source": [
    "# 统计聚类情况\n",
    "cluster_num = []\n",
    "cluster_index = []\n",
    "for cluster, n in clustered_list.items():\n",
    "    size = len(cluster)            \n",
    "    cluster_index.append(n[0])\n",
    "    cluster_num.append(size)\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'cluster': cluster_index,\n",
    "    'num': cluster_num\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)    \n",
    "\n",
    "df.to_csv('C:/Users/25031/Desktop/新建文件夹/dataset_pt2/_PUBLICO/A_wuhan_publico/TF+MCL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "691a2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "folder_path = 'C:/Users/25031/Desktop/新建文件夹/dataset_pt2/_PUBLICO/A_a_grecia_publico/time/'\n",
    "\n",
    "\n",
    "# 日期格式\n",
    "date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "# 所有文章的对应日期信息\n",
    "data_lists = []\n",
    "\n",
    "count = 0\n",
    "for count in range(len(vec_numpy)):\n",
    "    path = folder_path + str(count) + \".txt\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        data_list = [line.strip() for line in lines if date_pattern.match(line.strip())]\n",
    "        data_lists.append(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3b44c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-17']\n"
     ]
    }
   ],
   "source": [
    "print(data_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ac97f9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-17', '2020-03-18', '2020-03-22', '2020-03-24', '2020-03-31', '2020-04-05', '2020-04-11', '2020-04-23', '2020-04-25', '2020-05-30', '2020-06-04', '2020-06-07', '2020-06-13', '2020-06-29', '2020-07-06', '2020-07-28', '2020-08-04', '2020-08-13', '2020-08-18', '2020-09-02', '2020-09-10', '2020-10-22', '2020-10-25', '2020-10-27', '2020-10-29', '2020-10-31', '2020-11-05', '2021-01-02', '2021-01-08', '2021-01-12', '2021-01-15', '2021-01-29', '2021-02-09', '2021-02-24', '2021-03-03', '2021-03-17', '2021-03-23', '2021-03-23', '2021-04-12', '2021-04-13', '2021-04-19', '2021-04-22', '2021-04-25', '2021-04-29', '2021-05-14', '2021-05-18', '2021-05-29', '2021-06-18', '2021-06-23', '2021-07-12', '2021-07-13', '2021-07-13', '2021-07-17', '2021-07-24', '2021-07-26', '2021-07-30', '2021-07-30', '2021-08-23', '2021-08-24', '2021-08-26', '2021-09-11', '2021-11-30']\n"
     ]
    }
   ],
   "source": [
    "# Extract data from TLS DATASET\n",
    "import os\n",
    "folder_path = \"C:/Users/25031/Desktop/新建文件夹/dataset_pt2/_PUBLICO/A_a_grecia_publico/input_docs\"\n",
    "import os\n",
    "\n",
    "dct = []    # 用于存储文件名的列表\n",
    "\n",
    "# 遍历当前目录的所有子目录\n",
    "for subdir in os.listdir(folder_path):\n",
    "    subdir_path = os.path.join(folder_path, subdir)\n",
    "    # 如果是文件夹，则进一步遍历其下的文件\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, file_name)\n",
    "            # 确保是文件而不是文件夹\n",
    "            if os.path.isfile(file_path):\n",
    "                # 打开文件并读取内容\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    dct.append(file_name[0:10])\n",
    "\n",
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e9ee4e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2020-03-17', '2020-03-17'], ['2020-03-12', '2020-03-18'], ['2020-03-22', '2020-03-19', '2020-03-22', '2020-03-22'], ['2020-03-24'], ['2020-03-31', '2020-03-31']]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for count in range(len(vec_numpy)):\n",
    "    data_lists[count].append(dct[count])    \n",
    "    \n",
    "print(data_lists[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e27f96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, ['2020-03-17', '2020-03-17', '2020-06-13', '2020-06-15', '2020-06-15', '2020-06-13', '2021-04-12', '2021-04-12']]\n"
     ]
    }
   ],
   "source": [
    "date_all_cluster = []\n",
    "\n",
    "for n, cluster in clustered_list.items():\n",
    "    date_each_cluster = []\n",
    "    for article_num in cluster:\n",
    "        for date in data_lists[article_num]:\n",
    "            date_each_cluster.append(date[0:10])\n",
    "            \n",
    "    # cluster的编号对应日期list        \n",
    "    temp = []\n",
    "    temp.append(n+1)\n",
    "    temp.append(date_each_cluster)\n",
    "    date_all_cluster.append(temp)\n",
    "    \n",
    "print(date_all_cluster[0])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c02d4131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER :  6\n",
      "2020-03-17\n",
      "2\n",
      "CLUSTER :  15\n",
      "2020-04-11\n",
      "2\n",
      "CLUSTER :  4\n",
      "2020-03-22\n",
      "3\n",
      "CLUSTER :  5\n",
      "2021-01-12\n",
      "2\n",
      "CLUSTER :  14\n",
      "2021-01-15\n",
      "3\n",
      "CLUSTER :  12\n",
      "2020-04-05\n",
      "2\n",
      "CLUSTER :  8\n",
      "2020-04-23\n",
      "1\n",
      "CLUSTER :  3\n",
      "2021-03-23\n",
      "5\n",
      "CLUSTER :  9\n",
      "2020-06-05\n",
      "1\n",
      "CLUSTER :  11\n",
      "2020-10-27\n",
      "2\n",
      "CLUSTER :  2\n",
      "2020-08-18\n",
      "2\n",
      "CLUSTER :  1\n",
      "2021-01-02\n",
      "2\n",
      "CLUSTER :  13\n",
      "2021-03-23\n",
      "1\n",
      "CLUSTER :  10\n",
      "2021-04-04\n",
      "1\n",
      "CLUSTER :  16\n",
      "2021-04-29\n",
      "3\n",
      "CLUSTER :  17\n",
      "2021-05-18\n",
      "2\n",
      "CLUSTER :  7\n",
      "2021-07-29\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# 存取每个cluster对应的出现最多的日期和次数\n",
    "set_of_cluster_date = []\n",
    "for data in date_all_cluster:\n",
    "    key = data[0]\n",
    "    value = data[1]\n",
    "    counter = Counter(value)\n",
    "    most_common_element, count = counter.most_common(1)[0]\n",
    "    print(\"CLUSTER : \", key)\n",
    "    print(most_common_element)  \n",
    "    print(count)  \n",
    "    temp = {\"cluster\" : key, \"date\" : most_common_element, \"count\" : count}\n",
    "    set_of_cluster_date.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "216fca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster': 3, 'date': '2021-03-23', 'count': 5}\n",
      "{'cluster': 4, 'date': '2020-03-22', 'count': 3}\n",
      "{'cluster': 14, 'date': '2021-01-15', 'count': 3}\n",
      "{'cluster': 16, 'date': '2021-04-29', 'count': 3}\n",
      "{'cluster': 6, 'date': '2020-03-17', 'count': 2}\n",
      "{'cluster': 15, 'date': '2020-04-11', 'count': 2}\n",
      "{'cluster': 5, 'date': '2021-01-12', 'count': 2}\n",
      "{'cluster': 12, 'date': '2020-04-05', 'count': 2}\n",
      "{'cluster': 11, 'date': '2020-10-27', 'count': 2}\n",
      "{'cluster': 2, 'date': '2020-08-18', 'count': 2}\n",
      "{'cluster': 1, 'date': '2021-01-02', 'count': 2}\n",
      "{'cluster': 17, 'date': '2021-05-18', 'count': 2}\n",
      "{'cluster': 7, 'date': '2021-07-29', 'count': 2}\n",
      "{'cluster': 8, 'date': '2020-04-23', 'count': 1}\n",
      "{'cluster': 9, 'date': '2020-06-05', 'count': 1}\n",
      "{'cluster': 13, 'date': '2021-03-23', 'count': 1}\n",
      "{'cluster': 10, 'date': '2021-04-04', 'count': 1}\n"
     ]
    }
   ],
   "source": [
    "# 按照频率进行排序\n",
    "sorted_list = sorted(set_of_cluster_date, key=lambda x: x['count'], reverse=True)\n",
    "\n",
    "for item in sorted_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c4bb4bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 确认 gold_standard 里面有几个时间\n",
    "file_path = \"C:/Users/25031/Desktop/新建文件夹/dataset_pt/txt/a_grecia_publico/timelines/a_grecia.txt\"\n",
    "separator = \"--------------------------------\"\n",
    "with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "content = content.replace('\\n', '')\n",
    "data = content.split(separator)\n",
    "pro_data = []\n",
    "for d in data :\n",
    "    if d != '':\n",
    "        pro_data.append(d[0:10])\n",
    "    \n",
    "print(len(pro_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "15585327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "Precision :  0.11764705882352941\n",
      "F1 SCORE :  0.11764705882352941\n"
     ]
    }
   ],
   "source": [
    "# 对比 gold_standard 和 聚类结果\n",
    "num_of_tls = len(pro_data)\n",
    "\n",
    "cluster = sorted_list[0:num_of_tls]\n",
    "\n",
    "Golden = []\n",
    "for clus in cluster :\n",
    "    Golden.append(clus[\"date\"])\n",
    "\n",
    "print(len(Golden))    \n",
    "print(num_of_tls) \n",
    "\n",
    "#print(Golden)    \n",
    "#print(pro_data) \n",
    "\n",
    "set1 = set(Golden)\n",
    "set2 = set(pro_data)\n",
    "\n",
    "# 找出相同的日期\n",
    "common_elements = set1 & set2\n",
    "\n",
    "common_num = len(common_elements)\n",
    "\n",
    "precision = common_num/num_of_tls\n",
    "recall = common_num/num_of_tls\n",
    "print(\"Precision : \", precision)\n",
    "\n",
    "F1_score = (2*precision*recall)/(precision+recall)\n",
    "print(\"F1 SCORE : \", F1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9d1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da70040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f43b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
